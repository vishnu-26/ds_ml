{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f35d6ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tukura@email.com\n",
      "\n",
      "(123) 456-7890\n",
      "\n",
      "Seattle, WA\n",
      "\n",
      "Tasiana Ukura\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Fast  - Senior So\u0000ware Engineer\n",
      "October 2016 - current\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "Seattle, WA\n",
      "\n",
      "· Built and maintained application that scaled to 2M daily users, communicating with cross-functional\n",
      "\n",
      "teams regarding product and design\n",
      "\n",
      "· Transformed UIs using React, decreasing debugging time by 62% and increasing views by 31%\n",
      "· Focused on front-end development, providing mentorship and coaching to 6 interns each summer\n",
      "· Oversaw a team of 4 to write scalable code for the e-commerce platform that increase payment\n",
      "\n",
      "protection by 15%\n",
      "\n",
      "Adaptiva  - So\u0000ware Engineer\n",
      "May 2009 - October 2016\n",
      "\n",
      "Seattle, WA\n",
      "\n",
      "· Developed cloud-based technologies with C ++ and Java to assist Fortune 500 companies with scaling\n",
      "\n",
      "content distribution by 60% or more and increasing their productivity by 40% or more\n",
      "\n",
      "· Teamed up with current clients to understand needs for improved functionality, and communicated with\n",
      "\n",
      "engineers and clients to develop enhancements that boosted client satisfaction by 27%\n",
      "\n",
      "· Manipulated algorithms to align with marketing, sales, and solutions, improving automation by 32%\n",
      "· Drafted documentations delineating designs and specs for more than 20 projects\n",
      "\n",
      "Expedia Group - So\u0000ware Engineer Intern\n",
      "May 2008 - May 2009\n",
      "\n",
      "Seattle, WA\n",
      "· Worked with 5 other interns under the supervision of senior software engineer full stack development of\n",
      "\n",
      "the e-commerce system\n",
      "\n",
      "· Received coaching and support from peers and senior software engineer, and gained practical experience\n",
      "\n",
      "in using Java and Python\n",
      "\n",
      "· Studied data structures to recommend changes in algorithms, which boosted online sales by 6%\n",
      "· Partnered with interns, using code composition to redesign a clean API that offered increased ﬂexibility\n",
      "\n",
      "to to third parties, which generated a revenue increase of $1.5M\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "University of Washington - B.S., Computer Science\n",
      "August 2004 - May 2008\n",
      "\n",
      "Seattle, WA\n",
      "\n",
      "SKILLS\n",
      "Languages: Python, JavaScript, C++, Java; Frameworks: Django, NodeJS, React; Tools: jQuery, Unix, Git,\n",
      "Selenium; Databases: SQL (PostgreSQL, MySQL), NoSQL, AWS\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    " \n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    " \n",
    "text = ''\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('./resume1.pdf')\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fba6b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    " \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae4a5b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So\u0000ware Engineer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [[{'POS': 'PROPN'}, {'POS': 'PROPN'}]]\n",
    "    \n",
    "    matcher.add('NAME', pattern)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "\n",
    "print(extract_name(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9488f5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4567890\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_phone_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "\n",
    "print(extract_phone_number(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a5ccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tukura@email.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "emails = extract_emails(text)\n",
    "\n",
    "if emails:\n",
    "    print(emails[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "466cc90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SQL', 'Python', 'AWS'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    " \n",
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'data analysis',\n",
    "    'python',\n",
    "    'R',\n",
    "    'sql',\n",
    "    'hadoop',\n",
    "    'pyspark',\n",
    "    'statistics',\n",
    "    'maths',\n",
    "    'aws',\n",
    "    'azure',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "]\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    " \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    " \n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills\n",
    "\n",
    "skills = extract_skills(text)\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd3ba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WA', 'NodeJS', 'MySQL', 'Partnered', 'LinkedIn Seattle', 'Computer Science', 'JavaScript', 'Expedia Group', 'AWS', 'EDUCATION University', 'PostgreSQL', 'WA Tasiana Ukura', 'NoSQL', 'Senior'}\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    " \n",
    " \n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'university',\n",
    "    'academy',\n",
    "    'institute'\n",
    "]\n",
    "\n",
    "\n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    " \n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    " \n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word):\n",
    "                education.add(org)\n",
    " \n",
    "    return education\n",
    "\n",
    "education = extract_education(text)\n",
    "print(education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc29f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
